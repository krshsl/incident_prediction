{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook is the first step toward creating the input for our project\n",
    "\n",
    "\n",
    "__Main Steps__\n",
    "\n",
    "* _Libraries_: Run requirements.txt to install the required libraries. It is recommended to use a venv.\n",
    "\n",
    "* _Load traffic event data_: Traffic events taken from https://smoosavi.org/datasets/lstw - Please run the get_files.sh script to download the required files\n",
    "\n",
    "* _Load Sunlight data_: Sunlight data was downloaded from https://sunrise-sunset.org/, and it is downloaded/stored in this code\n",
    "\n",
    "* _Load Weather event data_: Weather events are downloaded dynamically from copernicus(default)/oikolab for specific lat and long. The copernicus data is obtained at 5km grid level, while oikolab can only be obtained at 25km level unless the user has a paid subscription.\n",
    "\n",
    "* _Construct Feature Vectors for pairs of City-Geohash_: This would be an initial feature csv that only contains time, traffic, and weather information "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T19:22:46.547529Z",
     "iopub.status.busy": "2025-04-27T19:22:46.546020Z",
     "iopub.status.idle": "2025-04-27T19:22:47.461274Z",
     "shell.execute_reply": "2025-04-27T19:22:47.460096Z"
    }
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool, cpu_count, Lock\n",
    "from urllib.request import Request, urlopen\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import requests\n",
    "import cdsapi\n",
    "import pprint\n",
    "import pytz\n",
    "import math\n",
    "import csv\n",
    "import ast\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T19:22:47.465751Z",
     "iopub.status.busy": "2025-04-27T19:22:47.465032Z",
     "iopub.status.idle": "2025-04-27T19:22:47.479708Z",
     "shell.execute_reply": "2025-04-27T19:22:47.478935Z"
    }
   },
   "outputs": [],
   "source": [
    "from common import cities, city_keys, years, months\n",
    "\n",
    "# defaulting to 1st date for both\n",
    "start_date = pd.to_datetime(f\"{years[0]}-{months[0]}-01 00:00:00\")\n",
    "end_date = pd.to_datetime(f\"{years[-1]}-{months[-1]}-01 00:00:00\")\n",
    "if start_date == end_date:\n",
    "    end_date = pd.to_datetime(f\"{years[-1]}-{months[-1]}-02 00:00:00\")\n",
    "    month_ranges = [f\"{years[0]}-{months[0]}-01/{years[-1]}-{months[-1]}-02\"]\n",
    "else:\n",
    "    month_ranges = [ # costly requests can take longer, might as well get one month at a time...\n",
    "        (f'{d.strftime('%Y-%m-01')}/{(d + pd.offsets.MonthBegin(1)).strftime('%Y-%m-01')}')\n",
    "        for d in pd.date_range(start=start_date, end=end_date, freq='MS') if d < end_date\n",
    "    ]\n",
    "\n",
    "class W_MODEL_API:\n",
    "    OIKO = \"OIKO\"\n",
    "    ECMFW = \"ECMFW\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T19:22:47.482605Z",
     "iopub.status.busy": "2025-04-27T19:22:47.481822Z",
     "iopub.status.idle": "2025-04-27T19:22:47.490387Z",
     "shell.execute_reply": "2025-04-27T19:22:47.489607Z"
    }
   },
   "outputs": [],
   "source": [
    "month_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T19:22:47.525076Z",
     "iopub.status.busy": "2025-04-27T19:22:47.524472Z",
     "iopub.status.idle": "2025-04-27T19:22:47.532048Z",
     "shell.execute_reply": "2025-04-27T19:22:47.531263Z"
    }
   },
   "outputs": [],
   "source": [
    "dirs = ['../data', '../data/traffic', '../data/weather', '../data/input', '../data/sample_daylight']\n",
    "for dir in dirs:\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "\n",
    "load_dotenv(dotenv_path=os.path.abspath('../prod.env'))\n",
    "api_key = os.environ['OIKOLAB_API']\n",
    "url = 'https://api.oikolab.com/weather' # this is paid, use at your own discretion\n",
    "weather_model = W_MODEL_API.OIKO\n",
    "\n",
    "max_cores = len(city_keys) if len(city_keys) < cpu_count() else cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T19:22:47.534493Z",
     "iopub.status.busy": "2025-04-27T19:22:47.533960Z",
     "iopub.status.idle": "2025-04-27T19:22:48.010838Z",
     "shell.execute_reply": "2025-04-27T19:22:48.009931Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(os.environ['HOME'],'.cdsapirc'), 'w') as file:\n",
    "    file.write(f'url: https://cds.climate.copernicus.eu/api\\n')\n",
    "    file.write(f'key: {os.environ['CDS_API']}\\n')\n",
    "\n",
    "\n",
    "weather_model = W_MODEL_API.ECMFW\n",
    "\n",
    "cdsClient = cdsapi.Client()\n",
    "\n",
    "# ecmwf_paramters = '141.128/164.128/165.128/166.128/167.128/228089.128/228090.128/228219.128'\n",
    "# [ # https://confluence.ecmwf.int/display/CKB/ERA5%3A+data+documentation#heading-Parameterlistings\n",
    "#     \"snow_depth\", #141 sd\n",
    "#     \"total_cloud_cover\", #164 tcc\n",
    "#     \"10m_u_component_of_wind\", #165 u10\n",
    "#     \"10m_v_component_of_wind\", #166 v10\n",
    "#     \"2m_temperature\", # 167 t2m\n",
    "#     \"total_column_rain_water\", #228089 tcrw\n",
    "#     \"total_column_snow_water\", #228089 tcsw\n",
    "#     \"large_scale_rain_rate\", #228219 lsrr\n",
    "#     \"large_scale_snowfall_rate_water_equivalent\" #228221 lssfr\n",
    "# ]\n",
    "\n",
    "ecmwf_variables = [\n",
    "    [\n",
    "        '2m_temperature',\n",
    "        '10m_u_component_of_wind',\n",
    "        '10m_v_component_of_wind',\n",
    "        'instantaneous_10m_wind_gust',\n",
    "        'snow_depth', \n",
    "        'total_cloud_cover'\n",
    "    ],\n",
    "    [\n",
    "        'total_precipitation'\n",
    "    ],\n",
    "    # [ # ignoring snowfall in favor of snow depth and precip?\n",
    "    #     'snowfall'\n",
    "    # ],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sunrise Sunset Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T19:22:48.013352Z",
     "iopub.status.busy": "2025-04-27T19:22:48.012954Z",
     "iopub.status.idle": "2025-04-27T19:22:56.230088Z",
     "shell.execute_reply": "2025-04-27T19:22:56.229286Z"
    }
   },
   "outputs": [],
   "source": [
    "for (city, city_vals) in cities.items():\n",
    "    file = open(f\"../data/sample_daylight/{city}_{years[0]}{months[0]}01_{years[-1]}{months[-1]}01.csv\", \"w\")\n",
    "    wr_csv = csv.writer(file)\n",
    "    is_header = False\n",
    "    s_month = 1\n",
    "    for year in range(years[0], years[-1]+1):\n",
    "        s_end = months[-1] if (len(years) > 1 and years[-1] == year) or (len(years)==1) else 12\n",
    "        s_start = months[0] if (s_month==1) else 1\n",
    "        s_month = 0\n",
    "        for month in range(s_start, s_end+1, 1):\n",
    "            req = Request(\n",
    "                url = f'https://sunrise-sunset.org/us/{city_vals['sunrise']}/{year}/{month}', \n",
    "                headers={'User-Agent': 'Mozilla/5.0'} # weirdly enough we get auth errors w/o this??\n",
    "            )\n",
    "            html = urlopen(req).read()\n",
    "            soup =  BeautifulSoup(html)\n",
    "            for span in soup.find_all(\"span\", {'class':'tooltip'}): \n",
    "                span.decompose()\n",
    "            \n",
    "            table = soup.select_one(\"table#month\")\n",
    "\n",
    "            if not is_header:\n",
    "                th_all = table.select(\"tr.headers th\")\n",
    "                headers = [th.text for th in th_all]\n",
    "                headers.append('Cities')\n",
    "                wr_csv.writerow(headers)\n",
    "                is_header = True\n",
    "            \n",
    "            tr_all = table.select(\"tr.day\")\n",
    "            day = 1\n",
    "            for tr in tr_all:\n",
    "                td_all = [td.text for td in tr.select(\"th,td\")]\n",
    "                td_all[0] = f'{year}-{month}-{day}'\n",
    "                day += 1\n",
    "                td_all.append(city)\n",
    "                wr_csv.writerow(td_all)\n",
    "\n",
    "    print(file.name)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the traffic events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T19:22:56.232642Z",
     "iopub.status.busy": "2025-04-27T19:22:56.232174Z",
     "iopub.status.idle": "2025-04-27T19:22:56.236074Z",
     "shell.execute_reply": "2025-04-27T19:22:56.235232Z"
    }
   },
   "outputs": [],
   "source": [
    "t_events = pd.read_csv('../TrafficEvents_Aug16_Dec20_Publish.csv') # This is the latest version of LSTW dataset\n",
    "# get the data from https://smoosavi.org/datasets/lstw\n",
    "\n",
    "t_events['StartTime(UTC)'] = pd.to_datetime(t_events['StartTime(UTC)'], utc=True)\n",
    "t_events['EndTime(UTC)'] = pd.to_datetime(t_events['EndTime(UTC)'], utc=True)\n",
    "t_events['StartTime'] = t_events['StartTime(UTC)']\n",
    "t_events['EndTime'] = t_events['EndTime(UTC)']\n",
    "t_events = t_events[(t_events['StartTime(UTC)'] >= start_date.tz_localize('UTC')) & (t_events['EndTime(UTC)'] < end_date.tz_localize('UTC'))]\n",
    "\n",
    "t_events.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean the traffic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T19:22:56.252379Z",
     "iopub.status.busy": "2025-04-27T19:22:56.251789Z",
     "iopub.status.idle": "2025-04-27T19:22:56.256157Z",
     "shell.execute_reply": "2025-04-27T19:22:56.255569Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_subset(city_vals):\n",
    "    tzone = pytz.timezone(city_vals['timezone'])\n",
    "    crds = city_vals['coordinates']\n",
    "    \n",
    "    subset_all = t_events[(t_events['LocationLat']>crds[0]) & (t_events['LocationLat']<crds[2]) & (t_events['LocationLng']>crds[1]) & \n",
    "                    (t_events['LocationLng']<crds[3])]\n",
    "    subset_all['StartTime'].dt.tz_convert(tzone)\n",
    "    subset_all['EndTime'].dt.tz_convert(tzone)\n",
    "    subset_all = subset_all.copy()\n",
    "    subset_all['Type'] = subset_all['Type'].str.replace('-', '')\n",
    "    \n",
    "    subset_accidents = subset_all[(subset_all['Type']=='Accident')]\n",
    "\n",
    "    return subset_all, subset_accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T19:22:56.258200Z",
     "iopub.status.busy": "2025-04-27T19:22:56.257654Z",
     "iopub.status.idle": "2025-04-27T19:22:56.261734Z",
     "shell.execute_reply": "2025-04-27T19:22:56.261094Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_traffic(city):\n",
    "    city_vals = cities[city]\n",
    "    subset_all, subset_accidents = get_subset(city_vals)\n",
    "    subset_all.to_csv(f'../data/traffic/TD_{city}_{years[0]}{months[0]}01_{years[-1]}{months[-1]}01.csv', index=False)\n",
    "    if len(subset_all):\n",
    "        print(f'For {city} we have {len(subset_all)} incidents, with {len(subset_accidents)} accidents! ratio {len(subset_accidents)*1.0/len(subset_all):.2f}')\n",
    "    return subset_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T19:22:56.263779Z",
     "iopub.status.busy": "2025-04-27T19:22:56.263223Z",
     "iopub.status.idle": "2025-04-27T19:22:56.266214Z",
     "shell.execute_reply": "2025-04-27T19:22:56.265600Z"
    }
   },
   "outputs": [],
   "source": [
    "tt_events = {}\n",
    "with Pool(max_cores) as p:\n",
    "    for i, event in enumerate(p.map(clean_traffic, city_keys)):\n",
    "        tt_events[city_keys[i]] = event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T19:22:56.268332Z",
     "iopub.status.busy": "2025-04-27T19:22:56.267779Z",
     "iopub.status.idle": "2025-04-27T19:22:56.270591Z",
     "shell.execute_reply": "2025-04-27T19:22:56.269957Z"
    }
   },
   "outputs": [],
   "source": [
    "for city in city_keys:\n",
    "    pprint.pprint(tt_events[city].head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T19:22:56.272565Z",
     "iopub.status.busy": "2025-04-27T19:22:56.272021Z",
     "iopub.status.idle": "2025-04-27T19:22:56.275018Z",
     "shell.execute_reply": "2025-04-27T19:22:56.274401Z"
    }
   },
   "outputs": [],
   "source": [
    "del t_events # major abuser of memory removed??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T19:22:56.277201Z",
     "iopub.status.busy": "2025-04-27T19:22:56.276738Z",
     "iopub.status.idle": "2025-04-27T19:22:56.286230Z",
     "shell.execute_reply": "2025-04-27T19:22:56.285603Z"
    }
   },
   "outputs": [],
   "source": [
    "class OIKO:\n",
    "    def __init__(self, city):\n",
    "        self.city = city\n",
    "\n",
    "    def get_weather_data(self, sdate, edate):\n",
    "        lat = []\n",
    "        long = []\n",
    "        city_vals = cities[self.city]\n",
    "\n",
    "        for (la, lo) in city_vals['weather']:\n",
    "            lat.append(la)\n",
    "            long.append(lo)\n",
    "        \n",
    "        response = requests.get(url,\n",
    "            params={'param': ['temperature', '10m_wind_gust', 'relative_humidity', 'wind_speed', 'wind_direction', 'total_cloud_cover', 'total_precipitation', 'snowfall'],\n",
    "                    'lat': lat,\n",
    "                    'lon': long,\n",
    "                    'start': sdate,\n",
    "                    'end': edate,\n",
    "                    'format': 'csv'},\n",
    "                    headers={'api-key': api_key}\n",
    "            )\n",
    "        \n",
    "        output = None\n",
    "        with tempfile.NamedTemporaryFile(mode='w+', delete=True) as tmp:\n",
    "            tmp.write(response.text)\n",
    "            output = pd.read_csv(tmp.name)\n",
    "            \n",
    "        \n",
    "        return output\n",
    "\n",
    "    def sample_weather(self):\n",
    "        tzone = pytz.timezone(cities[self.city]['timezone'])\n",
    "        traffic = pd.read_csv(f'../data/traffic/TD_{self.city}_{years[0]}{months[0]}01_{years[-1]}{months[-1]}01.csv')\n",
    "        if (len(traffic) == 0):\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # since weather is obtained based on utc, we use utc. shouldn't matter if the local time is different\n",
    "        traffic['StartTime(UTC)'] = pd.to_datetime(traffic['StartTime(UTC)'])\n",
    "        traffic = traffic.sort_values('StartTime(UTC)').set_index('StartTime(UTC)')\n",
    "        f_out = self.get_weather_data(traffic.index.min().date(), traffic.index.max().date())\n",
    "        f_out.columns = [col.partition(' ')[0] for col in f_out.columns] # ignore the suffix in col names\n",
    "        f_out['datetime'] = pd.to_datetime(f_out['datetime'], utc=True)\n",
    "        f_out['datetime(UTC)'] = f_out['datetime']\n",
    "        f_out['datetime'].dt.tz_convert(tzone)\n",
    "        f_out[['LocationLat', 'LocationLng']] = f_out['coordinates'].apply(lambda x: pd.Series(ast.literal_eval(x)) if isinstance(x, str) else pd.Series(x) if isinstance(x, (list, tuple)) and len(x) == 2 else pd.Series([None, None]))\n",
    "        f_out.drop(columns=['coordinates'], inplace=True)\n",
    "        f_out.to_csv(f'../data/weather/WD_{self.city}_{years[0]}{months[0]}01_{years[-1]}{months[-1]}01.csv')\n",
    "        return f_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T19:22:56.288430Z",
     "iopub.status.busy": "2025-04-27T19:22:56.287719Z",
     "iopub.status.idle": "2025-04-27T19:22:56.295548Z",
     "shell.execute_reply": "2025-04-27T19:22:56.294901Z"
    }
   },
   "outputs": [],
   "source": [
    "class ECMWF:\n",
    "    def __init__(self, city):\n",
    "        self.city = city\n",
    "\n",
    "    def get_weather_data(self):\n",
    "        coords = cities[self.city]['coordinates']\n",
    "        datasets = []\n",
    "        output_nc = []\n",
    "        output_dirs = []\n",
    "        index = 0\n",
    "        for date in month_ranges:\n",
    "            curr_var_set = []\n",
    "            for vars in ecmwf_variables:\n",
    "                output_dirs.append(tempfile.TemporaryDirectory(delete=True))\n",
    "                output_nc.append(os.path.join(output_dirs[index].name, f'era5_data{index}.nc'))\n",
    "                cdsClient.retrieve(\n",
    "                    'reanalysis-era5-single-levels',\n",
    "                    {\n",
    "                        'product_type': 'reanalysis',\n",
    "                        'format': 'netcdf',\n",
    "                        'variable': vars,\n",
    "                        'date': date,\n",
    "                        'time': '00/to/23/by/1',\n",
    "                        'area': coords,\n",
    "                        'grid': [0.05, 0.05],\n",
    "                        'data_format': 'netcdf'\n",
    "                    },\n",
    "                    output_nc[index]\n",
    "                )\n",
    "                curr_var_set.append(xr.open_dataset(output_nc[index], engine='netcdf4').load())\n",
    "                index += 1\n",
    "            \n",
    "            \n",
    "            datasets.append(xr.merge(curr_var_set))\n",
    "            del curr_var_set\n",
    "        \n",
    "        df = (xr.merge(datasets)).to_dataframe().reset_index()\n",
    "        for i in range(len(ecmwf_variables)*len(month_ranges)):\n",
    "            if i < len(output_nc):\n",
    "                os.remove(output_nc[i])\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        del datasets\n",
    "        return df\n",
    "\n",
    "    def sample_weather(self):\n",
    "        tzone = pytz.timezone(cities[self.city]['timezone'])\n",
    "        f_out = self.get_weather_data()\n",
    "        f_out['time'] = pd.to_datetime(f_out['valid_time'], utc=True)\n",
    "        f_out['time(UTC)'] = f_out['time']\n",
    "        f_out['time'].dt.tz_convert(tzone)\n",
    "        f_out.to_csv(f'../data/weather/WD_{self.city}_{years[0]}{months[0]}01_{years[-1]}{months[-1]}01.csv', index=False)\n",
    "        return f_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T19:22:56.297600Z",
     "iopub.status.busy": "2025-04-27T19:22:56.297159Z",
     "iopub.status.idle": "2025-04-27T19:22:56.300816Z",
     "shell.execute_reply": "2025-04-27T19:22:56.300111Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_weather_api(city):\n",
    "    if weather_model == W_MODEL_API.OIKO:\n",
    "        # for unit of measurements please refer https://docs.oikolab.com/parameters/\n",
    "        return OIKO(city)\n",
    "    else:\n",
    "        # rate limited to one request per user, so might as well perform single threaded operation\n",
    "        # for unit of measurements please refer https://cds.climate.copernicus.eu/datasets\n",
    "        return ECMWF(city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T19:22:56.302830Z",
     "iopub.status.busy": "2025-04-27T19:22:56.302406Z",
     "iopub.status.idle": "2025-04-27T19:22:56.305432Z",
     "shell.execute_reply": "2025-04-27T19:22:56.304873Z"
    }
   },
   "outputs": [],
   "source": [
    "def sample_weather(city):\n",
    "    return get_weather_api(city).sample_weather()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T19:22:56.307261Z",
     "iopub.status.busy": "2025-04-27T19:22:56.306939Z",
     "iopub.status.idle": "2025-04-27T19:22:56.309474Z",
     "shell.execute_reply": "2025-04-27T19:22:56.308911Z"
    }
   },
   "outputs": [],
   "source": [
    "wt_events = {}\n",
    "with Pool(max_cores) as p:\n",
    "    for i, event in enumerate(p.map(sample_weather, city_keys)):\n",
    "        wt_events[city_keys[i]] = event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T19:22:56.311294Z",
     "iopub.status.busy": "2025-04-27T19:22:56.310976Z",
     "iopub.status.idle": "2025-04-27T19:22:56.313702Z",
     "shell.execute_reply": "2025-04-27T19:22:56.313113Z"
    }
   },
   "outputs": [],
   "source": [
    "for city in city_keys:\n",
    "    print(city, wt_events[city].head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate final input csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T19:22:56.315691Z",
     "iopub.status.busy": "2025-04-27T19:22:56.315361Z",
     "iopub.status.idle": "2025-04-27T19:22:56.320892Z",
     "shell.execute_reply": "2025-04-27T19:22:56.320346Z"
    }
   },
   "outputs": [],
   "source": [
    "ttype_dict = {\n",
    "    'Construction': 0, # let's ignore insig events atm\n",
    "    'Event': 1,\n",
    "    'FlowIncident': 2,\n",
    "    'LaneBlocked': 3,\n",
    "    'BrokenVehicle': 4,\n",
    "    'Congestion': 5,\n",
    "    'Accident': 6\n",
    "}\n",
    "\n",
    "class Traffic_Event:\n",
    "    Severity: int = 0\n",
    "    StartTime: datetime = None\n",
    "    EndTime: datetime = None\n",
    "    LocationLat: float = 0\n",
    "    LocationLng: float = 0\n",
    "    Type: int = -1\n",
    "    # Accident: int = 0\n",
    "    # BrokenVehicle: int  = 0\n",
    "    # Congestion: int  = 0\n",
    "    # Construction: int  = 0\n",
    "    # Event: int  = 0\n",
    "    # FlowIncident: int  = 0\n",
    "    # LaneBlocked: int  = 0\n",
    "\n",
    "    def __init__(self, trafficRow, weatherRow, daylightRow):\n",
    "        self.Type = ttype_dict[trafficRow['Type']]\n",
    "        if (self.Type < 2):\n",
    "            return\n",
    "        self.StartTime = trafficRow['StartTime']\n",
    "        self.EndTime = trafficRow['EndTime']\n",
    "        self.Sunrise = daylightRow['Twilight Start']\n",
    "        self.Sunset = daylightRow['Twilight End']\n",
    "        # self.Severity = trafficRow['Severity'] # we can't predict severity, so it's alright i guess??\n",
    "        if (weather_model == W_MODEL_API.OIKO):\n",
    "            self.Snowfall = weatherRow['snowfall']\n",
    "            self.Precipitation = weatherRow['total_precipitation']\n",
    "            self.CloudCover = weatherRow['total_cloud_cover']\n",
    "            self.WindDirection = weatherRow['wind_direction']\n",
    "            self.WindSpeed = weatherRow['wind_speed']\n",
    "            self.RelativeHumidity = weatherRow['relative_humidity']\n",
    "            self.WindGust = weatherRow['10m_wind_gust']\n",
    "            self.Temperature = weatherRow['temperature']\n",
    "\n",
    "            ### I urge you to modify our code to make these parts work. We are only using ECMWF, so you might have to manually fix the grid\n",
    "            self.LocationLat = trafficRow['LocationLat']\n",
    "            self.LocationLng = trafficRow['LocationLng']\n",
    "        else:\n",
    "            self.LocationLat = weatherRow['latitude'] # closest 5km grid lat\n",
    "            self.LocationLng = weatherRow['longitude'] # closest 5km grid lng\n",
    "            self.SnowDept = weatherRow['sd']\n",
    "            # if 'sf' in weatherRow:\n",
    "            #     self.SnowFall = weatherRow['sf']\n",
    "            self.CloudCover = weatherRow['tcc']\n",
    "            self.UWind = weatherRow['u10']\n",
    "            self.VWind = weatherRow['v10']\n",
    "            if 'i10fg' in weatherRow:\n",
    "                self.Gust = weatherRow['i10fg']\n",
    "            self.Temperature = weatherRow['t2m']\n",
    "            if 'tp' in weatherRow:\n",
    "                self.Precipitation = weatherRow['tp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T19:22:56.322772Z",
     "iopub.status.busy": "2025-04-27T19:22:56.322412Z",
     "iopub.status.idle": "2025-04-27T19:22:56.325684Z",
     "shell.execute_reply": "2025-04-27T19:22:56.325141Z"
    }
   },
   "outputs": [],
   "source": [
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    lat1 = np.radians(lat1)\n",
    "    lon1 = np.radians(lon1)\n",
    "    lat2 = np.radians(lat2)\n",
    "    lon2 = np.radians(lon2)\n",
    "\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    return 6371000 * c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T19:22:56.327510Z",
     "iopub.status.busy": "2025-04-27T19:22:56.327192Z",
     "iopub.status.idle": "2025-04-27T19:22:56.332107Z",
     "shell.execute_reply": "2025-04-27T19:22:56.331526Z"
    }
   },
   "outputs": [],
   "source": [
    "file_lock = Lock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T19:22:56.333864Z",
     "iopub.status.busy": "2025-04-27T19:22:56.333692Z",
     "iopub.status.idle": "2025-04-27T19:22:56.336892Z",
     "shell.execute_reply": "2025-04-27T19:22:56.336327Z"
    }
   },
   "outputs": [],
   "source": [
    "def write_file(path, events: pd.DataFrame):\n",
    "    with file_lock:\n",
    "        curr_events = pd.DataFrame()\n",
    "        if os.path.exists(path) and os.path.getsize(path):\n",
    "            curr_events = pd.read_csv(path)\n",
    "        \n",
    "        if curr_events.empty or not curr_events.columns.equals(events.columns):\n",
    "            events.to_csv(path, index=False)\n",
    "        else:    \n",
    "            pd.concat([events, curr_events], ignore_index=True).to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T19:22:56.338977Z",
     "iopub.status.busy": "2025-04-27T19:22:56.338574Z",
     "iopub.status.idle": "2025-04-27T19:23:12.866873Z",
     "shell.execute_reply": "2025-04-27T19:23:12.866146Z"
    }
   },
   "outputs": [],
   "source": [
    "tt_events = {}\n",
    "wt_events = {}\n",
    "dl_events = {}\n",
    "for city in city_keys:\n",
    "    d_curr = pd.read_csv(f\"../data/sample_daylight/{city}_{years[0]}{months[0]}01_{years[-1]}{months[-1]}01.csv\")\n",
    "    d_curr['Twilight Start'] = pd.to_datetime(d_curr['Twilight Start'], format='%I:%M:%S %p').dt.strftime('%H:%M:%S')\n",
    "    d_curr['Twilight End'] = pd.to_datetime(d_curr['Twilight End'], format='%I:%M:%S %p').dt.strftime('%H:%M:%S')\n",
    "    d_curr['Day'] = pd.to_datetime(d_curr['Day']).dt.date\n",
    "    t_curr = pd.read_csv(f'../data/traffic/TD_{city}_{years[0]}{months[0]}01_{years[-1]}{months[-1]}01.csv')\n",
    "    t_curr['StartTime(UTC)'] = pd.to_datetime(t_curr['StartTime(UTC)'], utc=True)\n",
    "    t_curr['StartDay'] = t_curr['StartTime(UTC)'].dt.date\n",
    "    t_curr['EndTime(UTC)'] = pd.to_datetime(t_curr['EndTime(UTC)'], utc=True)\n",
    "    t_curr['StartTime(UTC)'] = t_curr['StartTime(UTC)'].apply(lambda x: x.replace(minute=0, second=0, microsecond=0))\n",
    "    t_curr['EndTime(UTC)'] = t_curr['EndTime(UTC)'].apply(lambda x: x.replace(minute=0, second=0, microsecond=0))\n",
    "    w_curr = pd.read_csv(f'../data/weather/WD_{city}_{years[0]}{months[0]}01_{years[-1]}{months[-1]}01.csv')\n",
    "    w_curr['time(UTC)'] = pd.to_datetime(w_curr['time(UTC)'], utc=True)\n",
    "    tt_events[city] = t_curr\n",
    "    wt_events[city] = w_curr\n",
    "    dl_events[city] = d_curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T19:23:12.869231Z",
     "iopub.status.busy": "2025-04-27T19:23:12.869048Z",
     "iopub.status.idle": "2025-04-27T19:23:12.873696Z",
     "shell.execute_reply": "2025-04-27T19:23:12.873213Z"
    }
   },
   "outputs": [],
   "source": [
    "def sample_input(args):\n",
    "    city = args[0]\n",
    "    start_i = args[1]\n",
    "    end_i = args[2]\n",
    "    file = args[3]\n",
    "    t_curr = tt_events[city]\n",
    "    w_curr = wt_events[city]\n",
    "    d_curr = dl_events[city]\n",
    "    \n",
    "    events = []\n",
    "    for t_index in range(start_i, min(len(t_curr), end_i)):\n",
    "        row = t_curr.iloc[t_index]\n",
    "        d_match = d_curr[(d_curr['Day'] == row['StartDay'])]\n",
    "        w_match = w_curr[(w_curr['time(UTC)'] == row['StartTime(UTC)'])].copy()\n",
    "        w_match['distance'] = haversine(w_match['latitude'], w_match['longitude'], row['LocationLat'], row['LocationLng'])\n",
    "        if w_match.empty or d_match.empty:\n",
    "            continue\n",
    "        \n",
    "        w_match = w_match[(w_match['distance'].min() == w_match['distance'])]\n",
    "        te = Traffic_Event(row, w_match.iloc[0], d_match.iloc[0])\n",
    "        if te.Type < 2:\n",
    "            continue\n",
    "        events.append(vars(te))\n",
    "    \n",
    "    if len(events) > 0:\n",
    "        write_file(file, pd.DataFrame(events))\n",
    "\n",
    "    del events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T19:23:12.875280Z",
     "iopub.status.busy": "2025-04-27T19:23:12.875147Z",
     "iopub.status.idle": "2025-04-27T19:32:54.662546Z",
     "shell.execute_reply": "2025-04-27T19:32:54.662003Z"
    }
   },
   "outputs": [],
   "source": [
    "with Pool(cpu_count()) as p:\n",
    "    for city in city_keys:\n",
    "        events = []\n",
    "        if (len(t_curr) == 0 or len(w_curr) == 0):\n",
    "            continue\n",
    "\n",
    "        file = f'../data/input/{city}_{years[0]}{months[0]}01_{years[-1]}{months[-1]}01.csv'\n",
    "        if os.path.exists(file):\n",
    "            os.remove(file)\n",
    "        per_cpu = len(tt_events[city])//cpu_count()\n",
    "        count = math.ceil(len(tt_events[city])/per_cpu)\n",
    "        args = [[city, per_cpu*i, per_cpu*(i+1), file] for i in range(count)]\n",
    "        print(city, count)\n",
    "        p.map(sample_input, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T19:32:54.665892Z",
     "iopub.status.busy": "2025-04-27T19:32:54.665647Z",
     "iopub.status.idle": "2025-04-27T19:32:54.668297Z",
     "shell.execute_reply": "2025-04-27T19:32:54.667986Z"
    }
   },
   "outputs": [],
   "source": [
    "# ## helper terminal??\n",
    "# # !echo '--- reading traffic events...'\n",
    "# # !head -n 3 TrafficEvents_Aug16_Dec20_Publish.csv\n",
    "# # !echo '--- reading weather events...'\n",
    "# # !head -n 3 WeatherEvents_Aug16_Dec20_Publish.csv\n",
    "# # !echo '--- traffic event types'\n",
    "# # !tail -n +2 TrafficEvents_Aug16_Dec20_Publish.csv | awk -F, '{print $2}' | sort | uniq\n",
    "# # !echo '--- weather event types'\n",
    "# # !tail -n +2 WeatherEvents_Aug16_Dec20_Publish.csv | awk -F, '{print $2}' | sort | uniq\n",
    "# # !echo '--- weather event severity'\n",
    "# # !tail -n +2 WeatherEvents_Aug16_Dec20_Publish.csv | awk -F, '{print $3}' | sort | uniq\n",
    "# # !tail -n+2 ../data/traffic/TD_Austin*.csv | awk -F, '{print $9,$10}' | sort | uniq | wc -l\n",
    "# # !tail -n+2 ../data/traffic/TD_Austin*.csv | wc -l\n",
    "\n",
    "\n",
    "# ## helper account details??\n",
    "# rr = requests.get('https://api.oikolab.com/account',\n",
    "#                  headers={'api-key': api_key}\n",
    "#                  )\n",
    "\n",
    "# rr.text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
