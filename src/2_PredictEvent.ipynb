{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook is the second step/final step toward predicting the event type\n",
    "\n",
    "\n",
    "\n",
    "__Main Steps__\n",
    "\n",
    "* _Libraries_: Run requirements.txt to install the required libraries if you haven't already. It is recommended to use a venv.\n",
    "\n",
    "* _Run first step to generate data_: Run 1_CreateInput.ipynb file to generate the input for our prediction model.\n",
    "\n",
    "* _Vectorize the data_: The csv data is encoded into vector format.\n",
    "\n",
    "* _LSTM Model_: The LSTM model is used to perform prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T19:33:01.986098Z",
     "iopub.status.busy": "2025-04-27T19:33:01.985826Z",
     "iopub.status.idle": "2025-04-27T19:33:07.481701Z",
     "shell.execute_reply": "2025-04-27T19:33:07.481053Z"
    }
   },
   "outputs": [],
   "source": [
    "from common import cities, city_keys, years, months\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding data for the model and splitting it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T19:33:07.484270Z",
     "iopub.status.busy": "2025-04-27T19:33:07.483905Z",
     "iopub.status.idle": "2025-04-27T19:33:07.488807Z",
     "shell.execute_reply": "2025-04-27T19:33:07.488291Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_sincos_column(df, key, data):\n",
    "    df[f'{key}_sin'] = np.sin(2 * np.pi * data)\n",
    "    df[f'{key}_cos'] = np.cos(2 * np.pi * data)\n",
    "\n",
    "def parse_time(df: pd.DataFrame):\n",
    "    time_cols = ['StartTime', 'EndTime', 'Sunrise', 'Sunset']\n",
    "    for column in time_cols:\n",
    "        if column in df.columns:\n",
    "            if not np.issubdtype(df[column].dtype, np.datetime64):\n",
    "                df[column] = pd.to_datetime(df[column])\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        if 'Sun' not in column:\n",
    "            days = df[column].dt.day\n",
    "            add_sincos_column(df, f'{column}_day', (days / 31))\n",
    "\n",
    "            months = df[column].dt.month\n",
    "            add_sincos_column(df, f'{column}_month', (months / 12))\n",
    "\n",
    "            years = df[column].dt.year\n",
    "            df[f'{column}_year_norm'] = (years - years.min()) / (years.max() - years.min())\n",
    "        \n",
    "        seconds = df[column].dt.hour * 3600 + df[column].dt.minute * 60 + df[column].dt.second\n",
    "        add_sincos_column(df, f'{column}_seconds', (seconds / 86400))\n",
    "\n",
    "    df = df.sort_values(by=['LocationLat', 'LocationLng', 'StartTime', 'EndTime'])\n",
    "    for column in time_cols:\n",
    "        df.drop(column, axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T19:33:07.490316Z",
     "iopub.status.busy": "2025-04-27T19:33:07.490126Z",
     "iopub.status.idle": "2025-04-27T19:33:07.492958Z",
     "shell.execute_reply": "2025-04-27T19:33:07.492466Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_data(df, train_fraction=0.8):\n",
    "    cut_index = int(len(df) * train_fraction)\n",
    "    train_df = df.iloc[:cut_index].reset_index(drop=True)\n",
    "    val_df = df.iloc[cut_index:].reset_index(drop=True)\n",
    "    return train_df, val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T19:33:07.494410Z",
     "iopub.status.busy": "2025-04-27T19:33:07.494221Z",
     "iopub.status.idle": "2025-04-27T19:33:07.497318Z",
     "shell.execute_reply": "2025-04-27T19:33:07.496812Z"
    }
   },
   "outputs": [],
   "source": [
    "def feature_scaling(X_train, X_val):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    return X_train_scaled, X_val_scaled, scaler\n",
    "\n",
    "def encode_labels(y_train, y_val):\n",
    "    label_enc = LabelEncoder()\n",
    "    y_train_enc = label_enc.fit_transform(y_train)\n",
    "    y_val_enc = label_enc.transform(y_val)\n",
    "    y_train_cat = to_categorical(y_train_enc)\n",
    "    y_val_cat = to_categorical(y_val_enc)\n",
    "    return y_train_cat, y_val_cat, label_enc, y_train_enc, y_val_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T19:33:07.498664Z",
     "iopub.status.busy": "2025-04-27T19:33:07.498528Z",
     "iopub.status.idle": "2025-04-27T19:33:07.501518Z",
     "shell.execute_reply": "2025-04-27T19:33:07.501023Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_sequences(X, y, seq_length):\n",
    "    sequences_X = []\n",
    "    sequences_y = []\n",
    "    for i in range(seq_length - 1, len(X)):\n",
    "        sequences_X.append(X[i - seq_length + 1: i + 1])\n",
    "        if y is not None:\n",
    "            sequences_y.append(y[i])\n",
    "    sequences_X = np.array(sequences_X)\n",
    "    sequences_y = np.array(sequences_y) if y is not None else None\n",
    "    return sequences_X, sequences_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and evaluvating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T19:33:07.503153Z",
     "iopub.status.busy": "2025-04-27T19:33:07.502850Z",
     "iopub.status.idle": "2025-04-27T19:33:07.506901Z",
     "shell.execute_reply": "2025-04-27T19:33:07.506356Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, input_shape=input_shape))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def train_model(model, X_train, y_train, X_val, y_val, epochs=50, batch_size=64):\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, \n",
    "                        validation_data=(X_val, y_val), verbose=1)\n",
    "    return history\n",
    "\n",
    "def save_model(model, file_path):\n",
    "    model.save(file_path)\n",
    "\n",
    "def load_saved_model(file_path):\n",
    "    return load_model(file_path)\n",
    "\n",
    "def evaluate_model(model, X_test, y_test_cat, y_test_enc, label_enc, file_name):\n",
    "    y_pred_probs = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
    "    y_true_classes = y_test_enc[len(y_test_enc) - len(y_pred_classes):]  # align sizes if needed\n",
    "    with open(file_name, \"w\") as file:\n",
    "        file.write(classification_report(y_true_classes, y_pred_classes, labels=np.arange(len(label_enc.classes_)), target_names=label_enc.classes_.astype(str)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating default directories if not exist already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T19:33:07.508508Z",
     "iopub.status.busy": "2025-04-27T19:33:07.508201Z",
     "iopub.status.idle": "2025-04-27T19:33:07.510863Z",
     "shell.execute_reply": "2025-04-27T19:33:07.510388Z"
    }
   },
   "outputs": [],
   "source": [
    "for path in ['../data/output', '../data/print']:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting each city individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T19:33:07.512507Z",
     "iopub.status.busy": "2025-04-27T19:33:07.512211Z",
     "iopub.status.idle": "2025-04-27T20:19:04.529175Z",
     "shell.execute_reply": "2025-04-27T20:19:04.528320Z"
    }
   },
   "outputs": [],
   "source": [
    "for city in city_keys:\n",
    "    file_path = f'../data/input/{city}_{years[0]}{months[0]}01_{years[-1]}{months[-1]}01.csv'\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    if df.empty:\n",
    "        print(f\"{city} is empty...??\")\n",
    "        continue\n",
    "\n",
    "    df = parse_time(df)\n",
    "    train_df, val_df = split_data(df)\n",
    "    X_train_df = train_df.drop('Type', axis=1)\n",
    "    y_train_series = train_df['Type']\n",
    "    X_val_df = val_df.drop('Type', axis=1)\n",
    "    y_val_series = val_df['Type']\n",
    "    X_train_scaled, X_val_scaled, scaler = feature_scaling(X_train_df.values, X_val_df.values)\n",
    "    y_train_cat, y_val_cat, label_enc, y_train_enc, y_val_enc = encode_labels(y_train_series.values, y_val_series.values)\n",
    "    \n",
    "    seq_length = 10\n",
    "    X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_cat, seq_length)\n",
    "    X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val_cat, seq_length)\n",
    "    \n",
    "    model = build_model(input_shape=(seq_length, X_train_seq.shape[2]), num_classes=y_train_cat.shape[1])\n",
    "    train_model(model, X_train_seq, y_train_seq, X_val_seq, y_val_seq, epochs=50, batch_size=64)\n",
    "    model_path = f\"../data/output/{city}_{years[0]}{months[0]}01_{years[-1]}{months[-1]}01.h5\"\n",
    "    print_path = f\"../data/print/{city}_{years[0]}{months[0]}01_{years[-1]}{months[-1]}01.txt\"\n",
    "    save_model(model, model_path)\n",
    "    evaluate_model(model, X_val_seq, y_val_seq, y_val_enc, label_enc, print_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treating all cities as a single entity for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T20:19:04.532186Z",
     "iopub.status.busy": "2025-04-27T20:19:04.531901Z",
     "iopub.status.idle": "2025-04-27T21:07:29.425217Z",
     "shell.execute_reply": "2025-04-27T21:07:29.424328Z"
    }
   },
   "outputs": [],
   "source": [
    "df = []\n",
    "for city in city_keys:\n",
    "    file_path = f'../data/input/{city}_{years[0]}{months[0]}01_{years[-1]}{months[-1]}01.csv'\n",
    "    df.append(pd.read_csv(file_path))\n",
    "\n",
    "if len(df) == 0:\n",
    "    print(f\"everything is empty...??\")\n",
    "    exit\n",
    "\n",
    "df = pd.concat(df, ignore_index=True)\n",
    "df = parse_time(df)\n",
    "train_df, val_df = split_data(df)\n",
    "X_train_df = train_df.drop('Type', axis=1)\n",
    "y_train_series = train_df['Type']\n",
    "X_val_df = val_df.drop('Type', axis=1)\n",
    "y_val_series = val_df['Type']\n",
    "X_train_scaled, X_val_scaled, scaler = feature_scaling(X_train_df.values, X_val_df.values)\n",
    "y_train_cat, y_val_cat, label_enc, y_train_enc, y_val_enc = encode_labels(y_train_series.values, y_val_series.values)\n",
    "\n",
    "seq_length = 10\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_cat, seq_length)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val_cat, seq_length)\n",
    "\n",
    "model = build_model(input_shape=(seq_length, X_train_seq.shape[2]), num_classes=y_train_cat.shape[1])\n",
    "train_model(model, X_train_seq, y_train_seq, X_val_seq, y_val_seq, epochs=50, batch_size=64)\n",
    "model_path = f\"../data/output/{years[0]}{months[0]}01_{years[-1]}{months[-1]}01.h5\"\n",
    "print_path = f\"../data/print/{years[0]}{months[0]}01_{years[-1]}{months[-1]}01.txt\"\n",
    "save_model(model, model_path)\n",
    "evaluate_model(model, X_val_seq, y_val_seq, y_val_enc, label_enc, print_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
